{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBNo4MgaFh80TssZkXR7lS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YoungPrzemoDev/AI-Projects/blob/main/BUILDING_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX732q7xcVZa"
      },
      "outputs": [],
      "source": [
        "#Attention is all you need\n",
        "#GPT - genratively pre-trained Transformer\n",
        "#Transformer - neural net that does heavy lifing under the hood"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TOKENIZACJA - zmiana tekstu jako string na intigery"
      ],
      "metadata": {
        "id": "E2Xz4gLJzp6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r1JmKKPzCdF",
        "outputId": "5fa4606a-8c51-4a60-9ea0-0f9b762ea2dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-10 13:44:47--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "input.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-03-10 13:44:47 (32.5 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "batch_size=32 # ile niezaleznych sekwencji bedzie procesowanych jednocześnie\n",
        "block_size = 8 # dlugosc contekstu do predykcji (context lenght)\n",
        "max_iters = 3000\n",
        "eval_interval=300\n",
        "learning_rate=1e-3\n",
        "eval_iters=200\n",
        "n_embd=32"
      ],
      "metadata": {
        "id": "IxoxX-c1vkuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#With - praca z zasobami, automatycznie otwiera i zamyka zasób\n",
        "# 'r' ze read\n",
        "with open ('input.txt','r',encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "8YelCFMizCuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total length in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57agaxlG19X0",
        "outputId": "4d97d144-6045-44a1-a2b6-84dbb761d86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total length in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Potrzebujemy posortowane znaki wiec uzywamy sorted ale funkcja sorted wymaga sekwencji np listy wiec dlatego jest\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBcUfE0j2ilZ",
        "outputId": "fe7f6cc5-a6c7-401b-96cc-65dc3bd08486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizer\n",
        "\n",
        "# klucz : wartosc for index, wartosc in enumerate(iterowalny_obiekt)\n",
        "# klucz : wartosc - to jest jakby nasz output, to otrzymujey po petli for, czyli akurat tutaj dictionary w pythonie ale to taka lista klucz wartosc\n",
        "stoi={ s:i for i,s in enumerate(chars) } # klameerki są do tworeznie słowników\n",
        "itos = { i:s for i,s in enumerate(chars)}\n",
        "encode = lambda s : [stoi[c] for c in s ]\n",
        "# List comprehension [itos[i] for i in l] przekształci [1, 2, 3] na ['a', 'b', 'c'].\n",
        "# A gdy dodamy ''.join() połączy te znaki w jeden ciąg znaków \"abc\" bez żadnego separatora,\n",
        "decode = lambda l : ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"Bunkrow nie ma, ale tez jest zajebiscie\"))\n",
        "print(decode(encode(\"Bunkrow nie ma, ale tez jest zajebiscie\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hLcG2p93BRB",
        "outputId": "574406b7-4d86-4051-bf16-52c2befd6cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14, 59, 52, 49, 56, 53, 61, 1, 52, 47, 43, 1, 51, 39, 6, 1, 39, 50, 43, 1, 58, 43, 64, 1, 48, 43, 57, 58, 1, 64, 39, 48, 43, 40, 47, 57, 41, 47, 43]\n",
            "Bunkrow nie ma, ale tez jest zajebiscie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "id": "WN6z9IgRv79V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data set\n",
        "n = int(0.9 * len(data)) # 90 prcoent\n",
        "train_data = data[:n] #do 90 prcoent\n",
        "val_data = data[n:] #po 90 procentach#"
      ],
      "metadata": {
        "id": "Yd7lHMJ1PcXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Podczas uczenia nie wkladamy calego tekstu tylko male kawałki bo computation za duzo przy calym\n",
        "train_data[:block_size + 1]\n",
        "\n",
        "#tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) - każdy token patrzy na wszystkie poprzednie tokeny aby lepiej zrozumieć jak powinien wyglądać nastepny token.\n",
        "#W tradycyjnych modelach sekwencyjnych, jak RNN, przewidywanie kolejnych tokenów odbywa się sekwencyjnie.\n",
        "#W modelu Transformera jednak wszystkie pozycje są przetwarzane równocześnie (w jednym przejściu przez model),\n",
        "#co jest dużo bardziej efektywne obliczeniowo ale nie robimy tego tylko po to. Równierz po to aby nasz transformer przywykł\n",
        "#do widzenia kontextu od najmniejszego aż do block size, że np wie po 1 znaku co ma byc dalej jako i po 5 czy 8 itp.\n",
        "#Potem zaczynamy obcinac i zaczynac kolejny block size bo przecież bedzie wiedział tylko do naszego block size"
      ],
      "metadata": {
        "id": "Ez9eCAZPzC-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Przykład tego jak każdy token patrzy na wszystkie poprzednie tokeny:\n",
        "x = train_data[:block_size]\n",
        "y = train_data[:block_size+1]\n",
        "for i in range(block_size):\n",
        "  context = x[:i+1] # do 1,2,3... indexu itp.\n",
        "  pred = y[i+1]\n",
        "  print(context, \" --> \", pred )"
      ],
      "metadata": {
        "id": "MDo3Ls6YBWxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDNr9w-4V_rC",
        "outputId": "89781539-ff2c-4e21-924e-7adf2bef1a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Batch\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "def getBatch(split):\n",
        "  if(split=='train'):\n",
        "    data = train_data\n",
        "  else:\n",
        "    data = val_data\n",
        "  #torch.randint(max_val, shape) -  tworzy tensor z random integers generowane uniformly between\n",
        "  #0 and the first argument a 2 argument to shape\n",
        "  #len(data) - zwraca rozmiar 1 wymiaru odpowiadającego za ilosc przykladow\n",
        "  #len(data)-block_size - Na przykład, jeśli data ma 1000 elementów, a block_size wynosi 100,\n",
        "  #to maksymalny indeks startowy, który możesz wybrać, to 900.\n",
        "  #W ten sposób ostatni wybrany blok danych będzie zawierał elementy od 900 do 999 włącznie, co zapewnia,\n",
        "  #że nie próbujesz sięgać po dane poza dostępnym zakresem bo randomo wybieramy z datasetu .\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,)) # dostajemy indexy tyle co batch_size ktore sa poczatkiem dla sekwencji danych block_size\n",
        "  #print(ix)\n",
        "\n",
        "  #stackujesz sobie jedno pod drugim , w sensie ze dokladasz wiersz\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb,yb = getBatch('train')\n",
        "print('inputs: ' , xb.shape)\n",
        "print(xb)\n",
        "print('targets: ' , yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('------')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b, :t+1]\n",
        "    target = yb[b,t]\n",
        "    print(f\"when input is{context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "id": "Mlh7bG4aTgs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Model):\n",
        "  def __init__(self,n_embd,n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd// n_head\n",
        "    #Communication\n",
        "    #self attention head. skoro 4 heady to C dzielimy na 4 bo kazdy bedzie pracowal na tych samych danych ale w innych wymiarach\n",
        "    self.sa = MultiheadAttention(n_head,head_size)\n",
        "    #Computation\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #Communication\n",
        "    #zanim przejdzie do sa i fe musimy znormalizwoac x\n",
        "    x = x + self.sa(self.ln1(x)) # x = x + self.sa(x) - to jest potrzebne do optymalizacji bardzo glebokich sieci i nazywa sie residual connections\n",
        "    #Computation\n",
        "    x = x + self.ffwd(self.ln2(x)) # x = x + self.ffwd(x) - to jest potrzebne do optymalizacji bardzo glebokich sieci i nazywa sie residual connections\n",
        "    return x"
      ],
      "metadata": {
        "id": "Yakdljs1qE0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "3ba8f9fd-95f2-4293-e521-d7e326c9c40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8dc63b0846f9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_embd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhead_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_embd\u001b[0m\u001b[0;34m//\u001b[0m \u001b[0mn_head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#Communication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Jesli ammy bardzo gleboka siec jak tu optymalizacja ubolewa.\n",
        "#Mozemy zrobic 2 rzeczy ktore pomoga"
      ],
      "metadata": {
        "id": "_QwpMsQ5sRUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn # importuje moudly i klasy\n",
        "from torch.nn import functional as F # importuje gotowe funkjce\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module): # dziedzicze po klasie Module\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__() # konstruktor Module\n",
        "    #Tworzymy tablice embedingow, Jest to tabela embeddingów dla tokenów\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
        "\n",
        "    #Jest to tabela embeddingów dla pozycji tokenów w sekwencji,\n",
        "    #Embedding pozycyjny ma na celu dostarczenie informacji o położeniu tokenu w sekwencji,\n",
        "    #co jest kluczowe dla modeli językowych, aby rozumieć kontekst i zależności między tokenami\n",
        "    #w zależności od ich pozycji.\n",
        "    #Przykładowo, słowo \"bank\" może mieć zupełnie inne znaczenie w zależności od tego, czy znajduje się\n",
        "    #po słowie \"rzeczny\" czy \"pieniężny\",\n",
        "    self.position_embedding_table = nn.Embedding(block_size,n_embd)\n",
        "    self.blocks = nn.Sequential(\n",
        "        Block(n_embd, h_head=4),\n",
        "        Block(n_embd, h_head=4),\n",
        "        Block(n_embd, h_head=4),\n",
        "        nn.layerNorm(n_embd), # na koncu transformera tez powinna byc normalizacja i tuz przed finalna liniowa ktroa przekształca potem w vocab_size\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embd,vocab_size)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None): # None by default bo przy generowaniu nie potrzebne\n",
        "    B, T = idx.shape #idx jest 2 wymiarowy\n",
        "\n",
        "    #Przy przekazaniu tensora idx do modelu, każda liczba całkowita (indeks) w tym tensorze\n",
        "    #odnosi się do wiersza w tabeli embeddingów i \"wybiera\" (ang. pluck out) z niej odpowiedni wiersz (embedding).\n",
        "    #Na przykład indeks 24 spowoduje wybranie 24-go wiersza z tabeli embeddingów.\n",
        "    token_emb = self.token_embedding_table(idx) # (B,T,C) batch, time , channel\n",
        "    #pozwala modelowi na uwzględnienie informacji o kolejności tokenów w sekwencji.\n",
        "    #arrange - Generuje embeddingi pozycyjne na podstawie sekwencji indeksów od 0 do T-1\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T)) #(T,C)\n",
        "    x = token_emb + pos_emb # (B,T,C) bo dzial ten broadcasting\n",
        "    x = self.sa_heads(x)\n",
        "    x = self.ffwd(x)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size), to jest fully conected linear layer\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    #idx jest w formie (B,T)\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "      #!!!!!!!!spradz jeszcze to uzywamy teraz jeszcze positiona embedings wiec musimy miec pewnosc ze otrzymamy tylko block_size anie wiecej\n",
        "      idx_cond = idx[:,-block_size:]\n",
        "\n",
        "      #slef(idx) uruchomienie forward i wez predykcje shape- (B,T,C)\n",
        "      logits, loss = self(idx_cond) #Dostaje tylko logity a loss ignorowane bo nie dostaje targeetów z ktorymi moze sobie porownac\n",
        "      logits = logits[:, -1, :]# becomes (B,C), interesuje nas ostatni krok czasowy bo chcemy predykcje zawsze dla ostatniego\n",
        "      #daj softmax zeby dostac prawdopodobienstwo\n",
        "      probs = F.softmax(logits, dim=1) # (B,C)\n",
        "      #sample from distribution\n",
        "      idx_next = torch.multinomial(probs,num_samples=1) # (B,1)\n",
        "      #concaconete to the running sequence\n",
        "      idx = torch.cat((idx, idx_next), dim=1) # (B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel()\n",
        "logits, loss = m(xb,yb) # podajemy targets wiec wykona sie else w forward\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "# tensor 1 na 1 i ma 0 czyli początek po prpstu 0 to new line character w naszym  slowniku\n",
        "# [0].tolits odnosi sie do calej sekwencji wygenerowanej\n",
        "print(decode(m.generate(torch.zeros((1,1), dtype = torch.long),100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgdGVH1218oR",
        "outputId": "a94ddb08-5e4d-4ae1-9381-662b0f15a2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 65])\n",
            "tensor(4.4519, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "awhbAGKFj;TjIyV,\n",
            "-vbyWjPEZQliW!XfWAwWafJ'GfOuv$ARqeemreKRRVemX!F$Xw!\n",
            "XvoyAIhdnCM3S;qR&AyyRoXoIjGSRpw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "EqcMtS4l4AWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for steps in range(1000):\n",
        "  xb,yb = getBatch('train')\n",
        "\n",
        "  #evaluate the loss\n",
        "  logits, loss = m(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True) #This clears old gradients from the last step\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMBxJS5uSB-r",
        "outputId": "090213c6-f70c-4121-e581-0f383fc2d1ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.506159543991089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(torch.zeros((1,1), dtype = torch.long),400)[0].tolist()))"
      ],
      "metadata": {
        "id": "vmr7WKpTVLqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mathematical trick in self attention"
      ],
      "metadata": {
        "id": "i5MXdqy81Lq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#self-attention is beiung used inside a Transformer i jest jak by sercem wydajności"
      ],
      "metadata": {
        "id": "4dBPCNq51LOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "#tensor o rozmairze podanym i randomowe liczby z rozkladu normalnego\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyhhlMhg1KVe",
        "outputId": "722ad084-1f56-46cf-f973-c6ac67afde51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Version 1\n",
        " xbow = torch.zeros(B,T,C)\n",
        " for b in range(B):\n",
        "  for t in range(T):\n",
        "    #Jak wybieram odpowiedni batch czyli B to redukuje wymiarowość,\n",
        "    xprev = x[b, :t+1] # (t,C)\n",
        "    #torch.mean(xprev,0) - zlicza nam mean xprev po pierwsszym wymiarze i przypisuje ja do tokenu znajdujacego sie na podanych indexach\n",
        "    xbow[b,t] = torch.mean(xprev,0)\n"
      ],
      "metadata": {
        "id": "yyMGIxQ03Koj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Version 2\n",
        "wei = torch.tril(torch.ones(T,T)) # daje 1 na przekatnej i pod nia a powyzej zera, potrzebne do oblcizenia mean\n",
        "wei = wei / wei.sum(1,keepdim=True) #(8,8), x = (4,8,2)\n",
        "# wei zostal rozciagniety o wymiar i ma teraz (1,8,8) zamiast (8,8) bo dziala mechanizm broadcastingu\n",
        "# dzial tak ze patrzy od konca na wymiary jest 8 i 2 potem 8 i 8 a 4 nie ma z czym poronwac wiec rozciaga o wymiar\n",
        "xbow2 = wei @ x # (1,8,8) @ (4,8,2) dziala bo nie patrzymy na 8 i 4  tylko 8 i 8 oraz 8 i 2 a piewszy eymiar jest rozciagany na 4 razy\n",
        "xbow2"
      ],
      "metadata": {
        "id": "90RTEo0JZPYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Version 3 using softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#zaczynaja sie do 0 wszędzie i myśl o nich jak o sile interakcji miedzy tokenami\n",
        "#one sa tylko na poczatku zero a potem zalezne od danych, tokeny beda patrzec po sobie i sprawdzac ktory ich bardziej interesuje\n",
        "wei = torch.zeros(T,T)\n",
        "#Tutaj mowimy ze tokeny z przeszlosci nie moga odwolywac sie do przyszlosci\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(wei.shape)\n",
        "#dim=1 kolumna wiec kolumna jest jest broadcastowana zeby otrzymac w weirszach sume do 1\n",
        "wei = F.softmax(wei,dim=1)  #exp() w softmax da kazde -inf na 0 bo tak dzial a 0 na 1 i potem zsumuje sobie wszystko do jednej kolumny i zrobi broadcasting\n",
        "xbow3 = wei @x\n",
        "xbow3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "GSqVXaHha58f",
        "outputId": "c32eb371-21d8-4bfb-a8b8-73f71353b5d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'T' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e11c49294cb1>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Version 3 using softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtril\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#zaczynaja sie do 0 wszędzie i myśl o nich jak o sile interakcji miedzy tokenami\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#one sa tylko na poczatku zero a potem zalezne od danych, tokeny beda patrzec po sobie i sprawdzac ktory ich bardziej interesuje\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "-"
      ],
      "metadata": {
        "id": "GEls9BRTiZF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W kontekście mechanizmu self-attention, klucze (keys) i zapytania (queries) pełnią centralną rolę w określaniu, jak każdy token (słowo lub inny element sekwencji) w sekwencji \"zwraca uwagę\" na inne tokeny. Oto, co każdy z nich robi i jak ze sobą współpracują:\n",
        "\n",
        "Klucze (Keys)\n",
        "\n",
        "    Rola: Klucze reprezentują tokeny w sekwencji w kontekście, na który mogą być zwrócone zapytania. Można je postrzegać jako etykiety lub identyfikatory, które pomagają określić, jakie informacje są dostępne dla zapytania.\n",
        "    Jak działają: Każdy token w sekwencji jest transformowany do przestrzeni kluczy za pomocą warstwy liniowej (w Twoim przykładzie key = nn.Linear(C, head_size, bias=False)), co pozwala na generowanie reprezentacji każdego tokenu jako klucz.\n",
        "\n",
        "Zapytania (Queries)\n",
        "\n",
        "    Rola: Zapytania są reprezentacjami tokenów, które \"pytają\" o inne tokeny w sekwencji, aby określić, jak bardzo powinny na nie zwracać uwagę. W praktyce oznacza to ustalanie, które tokeny są najbardziej istotne dla danego tokenu.\n",
        "    Jak działają: Podobnie jak klucze, każdy token jest transformowany do przestrzeni zapytań za pomocą innej warstwy liniowej (w Twoim przykładzie query = nn.Linear(C, head_size, bias=False)), generując reprezentację każdego tokenu jako zapytanie.\n",
        "\n"
      ],
      "metadata": {
        "id": "XWjPfLRnibkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Version 4 self-attention\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "#tensor o rozmairze podanym i randomowe liczby z rozkladu normalnego\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "#single head performance\n",
        "head_size = 16\n",
        "key = nn.Linear(C,head_size, bias = False)\n",
        "query = nn.Linear(C,head_size, bias = False)\n",
        "value = nn.Linear(C,head_size, bias = False)\n",
        "k = key(x) # (B,T,head_size)\n",
        "q = query(x) # (B,T,head_size)\n",
        "\n",
        "\n",
        "#wei reprezentuje iloczyn skalarny(Dot product) zapytania dla tokena i-tego i klucza dla tokena j-tego.\n",
        "#Iloczyn skalarny jest miarą podobieństwa: mówi nam, jak bardzo token reprezentowany przez zapytanie (query)\n",
        "#pasuje do tokenu reprezentowanego przez klucz (key).\n",
        "\n",
        "# wei jest przekazywane do softmax pozniej wiec gdy jedna wartosc jest znaczenie wieksza od pozostałych\n",
        "# wynikowy wektor prawdopodobieństw będzie bardzo bliski wektorowi \"one-hot\" czyli powiekszy roznice miedzy najwikeszym i mniejszymi wartosciami.\n",
        "# dltaego chcemy zaby wartosci byly ekstremalne\n",
        "# wiec robi sie wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
        "\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5 # (B,T,16) @ (B,16,T) --> (B,T,T)\n",
        "#przy mnozeni 3 wymiarowym popatrz tylko naostanie 2 wymiary (T,16) (16,T) wiec da nam T,T ale dla batcha jest (B,T,T)\n",
        "# (B,T,16) @ (B,16,T) --> (B,T,T) tutaj  to mnozenie (T,16) (16,T) odbywa sie rownolegle dla wszytskich przykladow przez Batch dimension\n",
        "# pierwsze T mowi o tym dla jakiego tokenu jest generowana uwaga\n",
        "# ostatnie T mowi o tym na jakich tokenach powinien sie skupić  gdy generuje uwage dla 1 Tokenu\n",
        "\n",
        "\n",
        "#tworzy trojkat z 1 na przekatnych i pod spodem\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "\n",
        "#mowimy ze tokeny z przyszlosci nie moga sie komunikowac.\n",
        "wei = wei.masked_fill(tril==0,float('-inf')) # kiedy jest prawdziwe w tensorze wartosc zostaje zastapiona tym co podalismy\n",
        "#tutaj dzieki softmax nie ma liczb ujemnych i dystrybucja sumuje sie do 1 w kazdym wierszu\n",
        "wei = F.softmax(wei,dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape\n"
      ],
      "metadata": {
        "id": "rIisLHbmbtY2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8b338096-71c3-4dc2-9a71-83d27cbeacad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bb204efba392>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Version 4 self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1337\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#tensor o rozmairze podanym i randomowe liczby z rozkladu normalnego\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Attention to mechanizm komunikacyjny\n",
        "\n",
        "#B - batch. pierwszy wymiar cyli wiersze. To są niezależne przykłądy które nie gadają ze sobą.\n",
        "#tylko w swoim przykladzie tokeny gadaja ze soba.\n",
        "\n",
        "#Attention to takie ogolne stwierzenie, jest self-attention i cross-attention\n",
        "#self-attention to kiedy keys queries i values pochodza od tego samego źródła\n",
        "#cross-attention to kiedy queries z jednego a queries i values od innego zrodla"
      ],
      "metadata": {
        "id": "0fLf0dOIIzc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "id": "gj2S_evMx_DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "wytlumacz mi to jak dla debila\n",
        "ChatGPT\n",
        "\n",
        "Oczywiście, spróbujmy to wyjaśnić w prostszy sposób, krok po kroku, używając analogii:\n",
        "\n",
        "Wyobraź sobie, że jesteś w dużym pokoju pełnym ludzi (tokenów) na przyjęciu (sekwencji). Każda osoba (każdy token) chce zdecydować, do kogo najbardziej należy zwrócić uwagę podczas rozmowy. W tym celu każdy używa trzech narzędzi: klucza, zapytania i wartości.\n",
        "\n",
        "    Zapytanie (Query): To Twoje osobiste pytanie. Wyobraź sobie, że trzymasz kartkę, na której napisane jest \"Jak bardzo powinienem zainteresować się każdą osobą tutaj?\". Twoje pytanie jest unikalne dla Ciebie i może być inne niż pytania innych osób.\n",
        "\n",
        "    Klucz (Key): Każda inna osoba na przyjęciu również trzyma swoją własną kartkę, na której jest napisane \"To są moje cechy, które mogą Cię zainteresować\". To jest ich klucz. Kiedy porównujesz swoje pytanie (query) z ich kluczem, możesz zdecydować, jak bardzo jesteś nimi zainteresowany.\n",
        "\n",
        "    Wartość (Value): Oprócz ich klucza, każda osoba ma również broszurę (to jest ich wartość), która zawiera więcej informacji o nich, na przykład ich historię życia, zainteresowania itd. Jeśli zdecydujesz, że ktoś jest interesujący na podstawie porównania Twojego pytania z ich kluczem, będziesz chciał poznać więcej z ich broszury.\n",
        "\n",
        "Teraz, jak to działa razem w self-attention:\n",
        "\n",
        "    Obliczenie uwagi: Korzystając z Twojego pytania, porównujesz je z kluczem każdej osoby, aby określić, jak ważna dla Ciebie jest każda z nich. To jest jakbyś wykonywał \"mnożenie\" swoich pytań przez ich klucze, aby określić, kto jest najbardziej związany z Twoim zainteresowaniem.\n",
        "\n",
        "    Stosowanie uwagi: Po ustaleniu, kto jest dla Ciebie najciekawszy, spojrzysz do ich broszur (wartości) i zbierasz z nich informacje. Ale nie czytasz każdej broszury w całości; zamiast tego, skupiasz się bardziej na tych osobach, które uznałeś za ważniejsze (czyli na tych, którym 'przydzieliłeś' więcej uwagi).\n",
        "\n",
        "    Tworzenie Twojej historii: Na koniec, bierzesz wszystkie informacje, które zebrałeś od innych (korzystając z ich wartości), ważysz je według tego, jak bardzo zainteresowały Cię ich klucze (czyli ile 'uwagi' im przydzieliłeś), i tworzysz swoją historię (czyli nową reprezentację siebie), która jest zsumowaniem tego, co dowiedziałeś się od innych i jak bardzo to dla Ciebie istotne.\n",
        "\n",
        "Podsumowując, w kontekście sieci neuronowych, \"zapytanie\" reprezentuje, jak ważne są dla Ciebie informacje z innych tokenów, \"klucz\" pokazuje, jak istotne mogą być informacje innych tokenów dla Ciebie, a \"wartość\" to faktyczne informacje, które chcesz zebrać od innych tokenów, bazując na tym, jak ważne są dla Ciebie według klucza i zapytania. Mechanizm uwagi (attention) pozwala każdemu tokenowi w sekwencji 'decydować', na których innych tokenach 'skupić uwagę', tworząc nową reprezentację opartą na całej dostępnej sekwencji."
      ],
      "metadata": {
        "id": "Ri1-udvVN4f4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3zwc1ZtFTMLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling."
      ],
      "metadata": {
        "id": "MEOf4Io7TLqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Scaled\" attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "yzWTRxGGTRUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "-#klasa Head zeby lepiej jakos to przedstawic\n",
        "\n",
        "class Head(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n.embd, head_size,bias = False)\n",
        "    self.query = nn.Linear(n.embd, head_size,bias = False)\n",
        "    self.value = nn.Linear(n.embd, head_size,bias = False)\n",
        "    #To jest akurat buffer i on nie ma gradientow jak by co tne tril\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size))) # tu jest maskymalna ilsoc jaka przetwarzam\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C = x.shape\n",
        "\n",
        "    k = self.key(x) # (B,T,C)\n",
        "    q = self.query(x) # (B,T,C)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0,5\n",
        "\n",
        "    #jesli masz juz zdefiniowane w konstruktorze wo musisz tylk podac\n",
        "    # odpowiednie wymiary maski kiedy rzeczywiste sekwencje, które przetwarzasz, są krótsze. [:T,:T]\n",
        "    # np przetwarzasz 8 na 8 a block size to 543\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] ==0,float('inf'))\n",
        "    wei = F.softmax(wei,dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out"
      ],
      "metadata": {
        "id": "pMTcli96XQBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#klasa MultiHead\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "  def __init__(self,num_heads,head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.projections = nn.Linera(n_embd,n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "#konkatonacja wynikow wszytwskich headow po wymiaerze C bo zostaly rozdzielone wczesniej\n",
        "  def forward(self,x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(out)\n",
        "    out = self.projections(out) # to jest powrót do sciezki rezydualnej\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "hq6F9_TGYsNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rozumiem, chcesz wiedzieć, jak w praktyce działa przetwarzanie danych przez głowy uwagi (attention heads) w modelu transformatora dla wielu batchy danych.\n",
        "\n",
        "Załóżmy, że masz tensor wejściowy o wymiarach [B, T, C], gdzie:\n",
        "\n",
        "    B = 16 (rozmiar batcha: liczba niezależnych sekwencji przetwarzanych równolegle),\n",
        "    T = 8 (długość sekwencji: liczba tokenów w każdej sekwencji),\n",
        "    C = 32 (wymiar cech: rozmiar embeddingu dla każdego tokenu).\n",
        "\n",
        "Ponadto, załóżmy, że masz n_head = 4 głowy uwagi w warstwie Multi-Head Attention.\n",
        "\n",
        "Oto jak to działa:\n",
        "\n",
        "    Rozdzielenie na Głowy Uwagi (Splitting into Attention Heads):\n",
        "    Każda \"głowa\" uwagi działa na części danych wejściowych. W praktyce, wymiar C jest dzielony równo na n_head głów, więc każda głowa pracuje z częścią cech każdego tokenu. Przy n_head = 4 i C = 32, każda głowa używa 8 cech z każdego tokenu (C / n_head = 32 / 4 = 8).\n",
        "\n",
        "    Przetwarzanie Batchy:\n",
        "        Dla każdego batcha danych (B = 16 sekwencji), wszystkie sekwencje są przetwarzane równolegle przez sieć.\n",
        "        Gdy mówimy o \"kilku batchach\", np. 30, oznacza to, że proces przetwarzania opisany poniżej jest powtarzany 30 razy, każdy raz z innym zestawem sekwencji.\n",
        "\n",
        "    Przebieg Przez Głowy Uwagi:\n",
        "        W ramach każdej głowy uwagi, dokonuje się przekształceń linearowych (za pomocą warstw nn.Linear) dla kluczy (k), zapytań (q) i wartości (v) z odpowiednich podziałów danych wejściowych. Dla każdej sekwencji w batchu, dla każdej głowy, obliczane są osobne zestawy k, q, v.\n",
        "        Następnie, dla każdej głowy i każdej sekwencji, obliczany jest iloczyn skalarny między zapytaniami a kluczami, a wynik jest skalowany, maskowany (jeśli to potrzebne, by zapobiec \"spojrzeniom w przyszłość\"), a na koniec stosowany jest softmax do określenia wag uwagi.\n",
        "        Wagi uwagi są używane do ważenia wartości (v) dla każdej głowy, co daje częściowe wyniki dla każdej głowy.\n",
        "\n",
        "    Łączenie Wyników z Głów (Combining Results from Heads):\n",
        "        Wyniki z wszystkich głów są następnie łączone (zwykle przez konkatenację wzdłuż wymiaru cech, a potem ewentualnie przekształcone przez dodatkową warstwę liniową), by zrekonstruować pełny wymiar C dla każdego tokenu.\n",
        "\n",
        "    Dalsze Przetwarzanie:\n",
        "        Wynik z warstwy uwagi wielogłowicowej może być następnie przetworzony przez pozostałe części bloku transformera, jak warstwy Feed-Forward, normalizację warstwową, itd.\n",
        "\n",
        "Podsumowując, w praktyce, dla wielu batchy danych, cały opisany proces jest powtarzany dla każdego batcha niezależnie. Każda \"głowa\" uwagi przetwarza wszystkie sekwencje w każdym batchu równolegle, ale skupia się na innych \"aspektach\" danych dzięki operowaniu na różnych podzbiorach cech. Mechanizm wielogłowicowej uwagi pozwala modelowi na równoczesne przetwarzanie różnych rodzajów relacji między tokenami w sekwencji."
      ],
      "metadata": {
        "id": "xilNVL33Rigp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rozumiem, chcesz zobaczyć konkretny przykład numeryczny. Załóżmy, że mamy następujące wymiary:\n",
        "\n",
        "    Rozmiar batcha (B) = 16,\n",
        "    Długość sekwencji (T) = 8,\n",
        "    Całkowita liczba cech (C) = 32,\n",
        "    Liczba głów uwagi (n_head) = 4,\n",
        "    Liczba cech na głowę (head_size) = C / n_head = 32 / 4 = 8.\n",
        "\n",
        "Rozpoczynając od tensora danych wejściowych o wymiarach [B, T, C], co w naszym przykładzie daje [16, 8, 32].\n",
        "\n",
        "    Operacja w Multi-Head Attention:\n",
        "\n",
        "    Na początku nie rozdzielamy jeszcze danych na głowy. Mamy tensor o wymiarach [16, 8, 32], co oznacza, że mamy 16 sekwencji, każda o długości 8 tokenów, i każdy token jest reprezentowany przez 32-wymiarowy wektor cech.\n",
        "\n",
        "    Transformacja dla poszczególnych głów:\n",
        "\n",
        "    Przygotowujemy dane dla każdej z głów uwagi. W modelach transformatorów, to zazwyczaj zachodzi wewnątrz operacji, które transformują oryginalne dane do przestrzeni kluczy (k), zapytań (q) i wartości (v) za pomocą osobnych warstw liniowych dla każdej głowy. Ale dla uproszczenia załóżmy, że już mamy nasze dane we właściwym formacie i chcemy tylko zrozumieć podział na głowy.\n",
        "\n",
        "    Zmiana kształtu danych:\n",
        "\n",
        "    Przekształcamy nasz tensor do nowego kształtu, który separuje głowy uwagi. To znaczy, przekształcamy z [B, T, C] do [B, n_head, T, head_size]. W naszym przypadku, to przekształcenie zmienia wymiary z [16, 8, 32] na [16, 4, 8, 8]. Oznacza to, że teraz każda z czterech głów ma do dyspozycji 8-wymiarowe reprezentacje dla każdego z 8 tokenów, i przetwarzamy 16 takich sekwencji równolegle.\n",
        "\n",
        "    Interpretacja nowego tensora:\n",
        "\n",
        "    W nowo przekształconym tensorze:\n",
        "        Pierwszy wymiar (16) wciąż reprezentuje rozmiar batcha,\n",
        "        Drugi wymiar (4) reprezentuje liczbę głów uwagi,\n",
        "        Trzeci wymiar (8) reprezentuje długość sekwencji (tyle samo co T),\n",
        "        Czwarty wymiar (8) reprezentuje nową liczbę cech na głowę (tyle samo co head_size).\n",
        "\n",
        "Każda \"głowa\" uwagi w modelu będzie teraz operować na swoim własnym, 8-wymiarowym podprzestrzeniach danych dla każdego tokenu, i to wszystko dzieje się równolegle dla wszystkich sekwencji w batchu. Po przetworzeniu przez mechanizmy uwagi, wyniki z różnych głów mogą być na różne sposoby integrowane, na przykład przez ponowne połączenie ich w większe reprezentacje i przeprowadzenie dalszych transformacji."
      ],
      "metadata": {
        "id": "zaitaby7RxJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feed forward -  to jest potrdzebne zeby siec pomyslala nad zebranymi danymui z self-attention\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # W pracy doktorackiej jest razy 4\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embd, n_embd),  # powrot do sciezki rezydualnej\n",
        "\n",
        "            #Dodajesz tuz przed zlaczeniem sie polaczen rezydualnych do orginalenej sciezki\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "2uMHMVLjbrRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Optymalizacja bardzo głebkoich sieci neuronowych:\n",
        "#Są 2 sposoby:\n",
        "#____PIERWSZY_____\n",
        "# Skip Connections (Połączenia Pomijające) zwane także Residual Connections (Połączenia Rezydualne):\n",
        "# Chodzi o to że masz dodatką droge dla gradientu bo przecież przy operacjach dodawania gradient jest po prostu przekazywany.\n",
        "#______DRUGI______\n",
        "#LayerNorm - layer normalization.\n",
        "#inna normalziacja niz batch normalization\n",
        "#W batch normalization normalizujemy po T czyli kolumnach czyli po cechach\n",
        "#W layer normalizujemy po wierszach czyli B czyli batchach\n",
        "#Podejscie sie zmienilo i teraz warstwa layernorm jest stosowana przed transformacja a nie po jak w pracy Attention\n"
      ],
      "metadata": {
        "id": "rqtTVzkhlcOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm:\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(0, keepdim=True) # batch mean\n",
        "    xvar = x.var(0, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]"
      ],
      "metadata": {
        "id": "-YLqLwlOsNX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Droppout - latwy sposob zeby zapobiec overfitingowi\n",
        "#za kazdym razem jak jest forward pass randomowo wybiera i wylacza jakies neurony\n",
        "#Jest to technika regularyzacji\n",
        "#np dropout = 0.2 kazdy forward pass 20 procent kalkulacji sa wywalane i zrzucane do 0."
      ],
      "metadata": {
        "id": "iuEyN2PYu0de"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYRAbaWPvtUD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}